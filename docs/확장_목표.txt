# 역할 분담 목표 — 무엇을 GPU에 둘까, 무엇을 CPU에 둘까
## GPU에 둘 것 (병렬성이 높고 timestep마다 자주 갱신되는 항목)
	- 구획별 전압(V), gating 변수(α/β 또는 m/h/n 등), 막전류 계산(HH 계열)
	- 수상돌기·축삭·soma의 compartment-level 연산 (local ionic currents, axial coupling)
	- 시냅스 전류 계산 (AMPA/NMDA conductance 업데이트), short-term plasticity(u, R 같은 STP 변수)
	- 스파이크 감지(문턱 비교), 스파이크 출력(스파이크 버퍼에 기록)
	- 지연 처리(딜레이 링 버퍼 / delay-stage spike queues)
	- 축삭 분기 라우팅(다수의 타깃에 대한 전송) + 확률적 전달(간단한 Bernoulli 실패 모델) — 단, RNG 효율/재현성 신경써야 함
	- 간단한 STDP 인크리먼트(혹은 근사된 가중치 업데이트) — 만약 실시간으로 처리해야 하고 병렬로 가능한 경우

## CPU에 둘 것 (비정기적이고 결정적·복잡한 로직)
	- 네트워크 토폴로지 변경(구조적 가소성: 연결 생성/삭제), long-term structural updates
	- 느리고 복잡한 plasticity(메타플라스틱성, consolidation, 복합 Ca-LTP 법칙 등) — GPU에선 근사만 처리
	- 고수준 이벤트 스케줄러/환경과의 닷-인터랙션(센서/행동 루프)
	- 시뮬레이션 체크포인트, 로깅, 디버깅, 시각화 인터페이스
	- 결정적 시드로 전체 시뮬레이션 재현성 관리 및 비교 테스트(참조 시뮬레이션 가능)

# 구체적 구현·성능/정확성 체크포인트 (주의할 것들)
## 이 항목들을 놓치면 설계는 유지되더라도 결과가 달라지거나 성능이 나빠짐.
1. 호스트-디바이스 통신 최소화
	- GPU ⇄ CPU 왕복을 timestep마다 하지 마. 대신 GPU에서 T_block (예: 10~100 simulation steps)를 연속으로 돌리고, 그 결과(스파이크 목록/요약)만 주기적으로 호스트에 전송해.
	- 전송은 비동기 cudaMemcpyAsync + pinned host memory + 별도 CUDA stream으로 오버랩.
2. 스파이크/이벤트 데이터 구조
	- GPU에선 spike_buffer(ring buffer)와 delay_stages(각 delay bucket에 접속되는 synapse 리스트/indices) 사용.
	- 압축: 스파이크는 비트마스크 또는 인덱스 리스트로 관리(네트워크 크기/밀도에 따라 선택).
	- Host에는 스파이크 리스트(소수의 이벤트)만 올려서 CPU가 토폴로지/PLASTICITY 로직 실행.
3. 데이터 레이아웃 — SoA 권장
	- GPU에서는 struct-of-arrays (V[], m[], h[], g_ampa[], ...)로 배열화해 coalesced memory를 보장하라.
	- 인덱스는 32-bit 정수, 전압/게이팅은 32-bit float(필요하면 테스트용으로 float64 대조).
4. 축삭 분기/전도 실패 구현 제안
	- 각 스파이크를 처리하는 axon_routing_kernel에서 타깃 목록(연결 오프셋 CSR: pre-> [start,end) )을 순회.
	- 실패는 per-target Bernoulli: if (curand_uniform(&s) < p_release) apply_post_synapse()
	- RNG: Philox/Philox4x32 또는 Xorshift with per-thread seed derived from (neuron_id, step)로 재현성 확보.
	- 전송 지연은 delay ring에 (post_index, weight, syn_type) push.
5.딜레이 처리 (delay ring)
	- discrete-time delay로 처리: ring[ (current_step + delay_ticks) % ring_size ].append(spike_event)
	- 각 ring bucket은 dynamic-length list — 구현은 fixed-size scatter/gather: 먼저 count, prefix-sum, then fill (two-pass) 혹은 atomic append with preallocated buffers.
6. Plasticity / STDP
	- STP (facil/depress)는 presynaptic event마다 업데이트 가능 → GPU에서 처리하기 좋음.
	 -STDP (timing-based)도 GPU에서 가능: store last_spike_time per neuron; on pre/post spike, compute delta and apply weight update (atomic add).
	- 그러나 복잡한 Ca-LTP/structural plasticity는 CPU에 맡기고 주기적으로 반영(바꿀 연결 목록을 CPU가 보내면 GPU에서 반영).
7. 수치 통합법
	- Euler는 간단하지만 안정성/정확성 문제. GPU에서 step size 작게 쓰거나 exp-integrator / RK2로 교체 권장.
	- 만약 dt가 작아져야 성능 저하가 심하면, kernel 내부에서 multiple small steps를 묶어 처리.
8. 디버깅/검증
	- 작은 네트워크(1~10 뉴런)로 CPU 구현과 GPU 결과를 step-by-step 비교.
	- float 정밀도 차이 때문에 완전 동일 기대하지 말고 L2 norm/스파이크 타이밍 유사성으로 검증.
9. 동기화 & 레이스
	- 동시 weight 업데이트(atomic)가 많아지면 성능 저하. 가능하면 per-thread-block reduction하고 block-level 업데이트로 병합해서 atomic 빈도 줄이기.
	- 큰 변화(연결 삭제/추가)는 CPU에서 일괄 반영하는 것이 안전(동적 재할당/압축 필요).
10. 확률성(reproducibility)
	- GPU RNG는 non-deterministic일 수 있음(스레드 스케줄에 따라). 재현성이 필요하면 Philox같은 counter-based RNG를 사용해 (step, neuron_id)를 counter로 해서 결정론적 난수 생성.

# 실전적 데이터 흐름(권장 워크플로우, pseudocode)
## 간단한 루프 구조(한 iteration = gpu_steps timesteps):
```
CPU:
  while sim_running:
    launch kernel simulate_steps(device_state, gpu_steps)  // GPU computes volt/gates, collects spikes into device_spike_buffer
    cudaMemcpyAsync(device_spike_buffer -> host_pinned_spike_buf)
    // while copy in flight, CPU can handle slow housekeeping (logging, environment I/O)
    cudaEventSynchronize(spike_copy_event)
    process host_pinned_spike_buf:  // scheduling, structural plasticity decisions, neuromodulator effects
      update network/topology if needed
      form commands: {weight_changes, new_connections, control_flags}
    upload commands to GPU via cudaMemcpyAsync to device_control_buf
    loop
```
## GPU 내부(커널 구성 권장):
	- kernel A: integrate_compartments(dt, n_steps) — 반복적으로 V/gates update; detect spikes and write into per-step spike ring
	- kernel B: process_delay_and_synapses() — pull events from ring[(t)%ring_size], for each event:
use CSR to iterate postsynapses
sample release (if probabilistic)
add conductance to post compartment (atomic)
update STP variables
	- kernel C (periodic): axon_routing_kernel if you separate axon logic.
이렇게 구성하면 GPU 내부 작업은 최대한 디바이스에서 끝나고, CPU ↔ GPU는 spike summary 및 제어만 주고받음.

# 구체적인 자료구조 제안 (간단한 예)
	- Neuron arrays (SoA): V[nNeurons], m[nNeurons], h[nNeurons], n[nNeurons], last_spike_time[nNeurons]
	- Compartment arrays (if neuron has multiple compartments, flatten each compartment as unique id): compV[nCompartments], comp_gna[], comp_gk[], comp_area[], comp_to_neuron[]
	- Synapse CSR:
		pre_index[] (implicit via row_ptr)
		row_ptr[nPre+1] (start/end offsets)
		col_idx[nSyn] (post compartment index)
		weight[nSyn], delay_ticks[nSyn], p_release[nSyn], syn_type[nSyn]
	- Delay ring: delay_bucket[ring_size][max_events_per_bucket] or dynamic via event_storage[] with bucket_head[]/bucket_count[]

# RNG/확률 처리 팁
	- RNG per event: Philox/counter-based generator (cuRAND Philox) -> deterministic if you use (neuron_id, global_step) as counter.
	- Avoid storing big RNG states per synapse (메모리 폭발). Counter-based RNG는 state 저장 불필요.

# 성능 튜닝 포인트 (우선순위)
	1. 메모리 coalescing(SoA) 및 정렬(align)
	2. atomic 사용 최소화 — block-level aggregation 사용
	3. delay ring 구현 효율화(두-패스, prefix-sum으로 GPU-side packing)
	4. 비동기 복사 & 스트림 오버랩으로 전송 대기 줄이기
	5. 프로파일링(Nsight)으로 병목 확인 — 커널별 warp 효율, memory-bound vs compute-bound 판단

# 안정성·디버깅 전략
	- 단계적 포팅: 먼저 단일 compartment HH와 단순 synapse (deterministic) → 맞는지 검증 → stochastic release 추가 → delay ring 추가 → STP → STDP → 구조적 가소성(최종).
	- 작은 네트워크(예: 1 neuron / 1 pre-post synapse)로 CPU vs GPU 결과 비교.
	- GPU 출력(스파이크 타임스탬프)과 CPU 레퍼런스 비교: spike-time jitter 허용 범위 정의.

# 실무적 권장 우선작업 (바로 적용하면 좋은 것)
	1. GPU에 delay ring + axon routing (with p_release) 구현 — relatively low complexity, 동작 재현성 크게 향상.
	2. GPU에서 STP(근사) 추가 — synaptic response 모사에 큰 효과.
	3. GPU 내부에서 spike export buffer 형태를 정하고, host↔device 전송을 비동기로 바꿔서 실험.
	4. CPU는 현재 pyramidal_neuron.py 구조 유지하면서, GPU에서 온 spike summary를 받아 구조적 가소성, neuromodulation, 복잡한 plasticity를 처리하도록 구현.

# 목표 프로젝트 구조
src/
├─ kernels/
│  ├─ neuron_kernels.cu       # compartment-level HH update, spike detection
│  ├─ synapse_kernels.cu      # synaptic conductance update, STP
│  ├─ axon_kernels.cu         # axon routing, delay ring 처리
│  ├─ plasticity_kernels.cu   # STDP / 근사 plasticity
│  ├─ utils_kernels.cu        # RNG, prefix-sum, scatter/gather
│  └─ common.h                # GPU/CPU 공용 struct/enum/constant 정의
│
├─ gpu/
│  ├─ neuron_ops.py           # neuron kernel 호출 래퍼
│  ├─ synapse_ops.py          # synapse kernel 호출 래퍼
│  ├─ axon_ops.py             # axon kernel 호출 래퍼
│  ├─ plasticity_ops.py       # plasticity kernel 호출 래퍼
│  └─ gpu_utils.py            # 커널 컴파일, CUDA stream, pinned memory, async copy, RNG 관리
│
├─ cpu/
│  ├─ network_manager.py      # 전체 시뮬레이션 루프, CPU 매니저
│  ├─ topology.py             # neuron/synapse 구조 정의, CSR/connection
│  ├─ plasticity.py           # 복잡한 plasticity, Ca-LTP, consolidation
│  ├─ stimulus.py             # 외부 자극/환경 인터페이스
│  ├─ logging.py              # 시뮬레이션 기록, 체크포인트
│  └─ visualization.py        # 전압, spike raster plot, 디버깅 UI
│
├─ neuron_structs.py          # Neuron/Compartment SoA 정의
├─ synapse_structs.py         # Synapse CSR, weight, delay, p_release 등
├─ config.py                  # 시뮬레이션 파라미터 (dt, g_na/g_k 등)
│
docs/                         # 문서, 개발 노트, 설계 설명
├─ pyramidal_neuron.py  # 기초 설계
├─ 누락된_기능.txt
├─ NewNeuroSysWorks.txt
└─ 확장_목표.txt (이 파일)